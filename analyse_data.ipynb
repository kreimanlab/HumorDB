{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Processed Dataset from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humor_db = load_dataset(\"VedaantJain/HumorDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humor_db['train'][0], humor_db['validation'][0], humor_db['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "HF_TOKEN = \"\"\n",
    "headers = {\"Authorization\": f\"{HF_TOKEN}\"}\n",
    "API_URL = \"https://huggingface.co/api/datasets/VedaantJain/HumorDB/croissant\"\n",
    "def query():\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "    return response.json()\n",
    "data = query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Raw Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Binary and Range Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'Dataset'\n",
    "dataset_images = []\n",
    "train_images = []\n",
    "valid_images = []\n",
    "test_images = []\n",
    "train_funny = []\n",
    "train_not_funny = []\n",
    "valid_funny = []\n",
    "valid_not_funny = []\n",
    "test_funny = []\n",
    "test_not_funny = []\n",
    "for fold in ['Train', 'Valid', 'Test']:\n",
    "    for fold1 in ['Funny', 'Not_Funny']:\n",
    "        for file in os.listdir(os.path.join(dataset_dir, fold, fold1)):\n",
    "            dataset_images.append(file)\n",
    "            if fold == 'Train':\n",
    "                train_images.append(file)\n",
    "                if fold1 == 'Funny':\n",
    "                    train_funny.append(file)\n",
    "                else:\n",
    "                    train_not_funny.append(file)\n",
    "            elif fold == 'Valid':\n",
    "                valid_images.append(file)\n",
    "                if fold1 == 'Funny':\n",
    "                    valid_funny.append(file)\n",
    "                else:\n",
    "                    valid_not_funny.append(file)\n",
    "            else:\n",
    "                test_images.append(file)\n",
    "                if fold1 == 'Funny':\n",
    "                    test_funny.append(file)\n",
    "                else:\n",
    "                    test_not_funny.append(file)\n",
    "print('Dataset size:', len(dataset_images))\n",
    "print('Train size:', len(train_images), len(train_funny), len(train_not_funny))\n",
    "print('Valid size:', len(valid_images), len(valid_funny), len(valid_not_funny))\n",
    "print('Test size:', len(test_images), len(test_funny), len(test_not_funny))\n",
    "\n",
    "#sanity check\n",
    "for file in train_images:\n",
    "    assert file not in valid_images\n",
    "    assert file not in test_images\n",
    "\n",
    "for file in valid_images:\n",
    "    assert file not in train_images\n",
    "    assert file not in test_images\n",
    "\n",
    "for file in test_images:\n",
    "    assert file not in train_images\n",
    "    assert file not in valid_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_words = {}\n",
    "with open('image_common_words.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            image, words = line.split(',')\n",
    "            image_words[image] = words.strip().split(' ')\n",
    "len(image_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_binary_ratings(file_path):\n",
    "    user_ratings = {}\n",
    "    count_diff = 0\n",
    "    num_repeat = 0\n",
    "    all_ratings = []\n",
    "    with open(os.path.join(file_path)) as f:\n",
    "        lines = f.readlines()\n",
    "        i = 4\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.strip()\n",
    "            assert ',' in line or i == len(lines) - 1\n",
    "            line = line.split(',')\n",
    "            assert line[1] in ['0', '1']\n",
    "            all_ratings.append(line)\n",
    "            if line[0].strip() in user_ratings:\n",
    "                num_repeat += 1\n",
    "                if user_ratings[line[0].strip()] != int(line[1].strip()):\n",
    "                    count_diff += 1\n",
    "            else:\n",
    "                user_ratings[line[0].strip()] = int(line[1].strip())\n",
    "            i += 1\n",
    "    fir_len = len(all_ratings[0])\n",
    "    for i in range(len(all_ratings)):\n",
    "        if len(all_ratings[i]) != fir_len:\n",
    "            print('fix_len', file_path)\n",
    "            break\n",
    "    if count_diff > 3:\n",
    "        print(file_path, count_diff)\n",
    "    return user_ratings, count_diff, num_repeat\n",
    "\n",
    "def read_range_ratings(file_path):\n",
    "    user_ratings = {}\n",
    "    count_diff = 0\n",
    "    num_repeat = 0\n",
    "    with open(os.path.join(file_path)) as f:\n",
    "        lines = f.readlines()\n",
    "        i = 4\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            line = line.strip()\n",
    "            assert ',' in line or i == len(lines) - 1\n",
    "            line = line.split(',')\n",
    "            if line[0].strip() in user_ratings:\n",
    "                num_repeat += 1\n",
    "                assert np.abs(user_ratings[line[0]] - int(line[1])) <= 4\n",
    "                if np.abs(user_ratings[line[0].strip()] - int(line[1].strip())) > 2:\n",
    "                    count_diff += 1\n",
    "            else:\n",
    "                user_ratings[line[0].strip()] = int(line[1].strip())\n",
    "            i += 1\n",
    "        \n",
    "    if count_diff > 4:\n",
    "        print(file_path, count_diff)\n",
    "    return user_ratings, count_diff, num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dir = 'user_binary'\n",
    "range_dir = 'user_range'\n",
    "\n",
    "# for i, file in enumerate(os.listdir(binary_dir)):\n",
    "#     os.rename(os.path.join(binary_dir, file), os.path.join(binary_dir, f'user_{i}.txt'))\n",
    "\n",
    "# for i, file in enumerate(os.listdir(range_dir)):\n",
    "#     os.rename(os.path.join(range_dir, file), os.path.join(range_dir, f'user_{i}.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_binary_ratings = {}\n",
    "all_range_ratings = {}\n",
    "for file in os.listdir(binary_dir):\n",
    "    user_ratings, count_diff, num_repeat = read_binary_ratings(os.path.join(binary_dir, file))\n",
    "    if num_repeat <= 0:\n",
    "        print(f\"Number of repeats for {file} is {num_repeat}\")\n",
    "    \n",
    "    for key, value in user_ratings.items():\n",
    "        if key in all_binary_ratings:\n",
    "            all_binary_ratings[key].append(value)\n",
    "        else:\n",
    "            all_binary_ratings[key] = [value]\n",
    "\n",
    "for file in os.listdir(range_dir):\n",
    "    user_ratings, count_diff, num_repeat = read_range_ratings(os.path.join(range_dir, file))\n",
    "    if num_repeat <= 0:\n",
    "        print(f\"Number of repeats for {file} is {num_repeat}\")\n",
    "    \n",
    "    for key, value in user_ratings.items():\n",
    "        if key in all_range_ratings:\n",
    "            all_range_ratings[key].append(value)\n",
    "        else:\n",
    "            all_range_ratings[key] = [value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sanity\n",
    "for key, value in all_binary_ratings.items():\n",
    "    if key not in dataset_images:\n",
    "        continue\n",
    "    if len(value) < 5:\n",
    "        print(f\"Img {key} has less than 5 ratings in binary ratings\")\n",
    "    if len(all_range_ratings[key]) < 5:\n",
    "        print(f\"Img {key} has less than 5 ratings in range ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ratings = {}\n",
    "range_ratings_mean = {}\n",
    "count = 0\n",
    "for key, value in all_range_ratings.items():\n",
    "    new_value = []\n",
    "    mean = np.mean(value)\n",
    "    std = np.std(value)\n",
    "    for v in value:\n",
    "        if np.abs((v - mean)/std > 1.95):\n",
    "            continue\n",
    "        new_value.append(v)\n",
    "    assert len(new_value) >= 1\n",
    "    range_ratings[key] = new_value\n",
    "    range_ratings_mean[key] = np.mean(new_value)\n",
    "\n",
    "binary_ratings = {}\n",
    "binary_ratings_mean = {}\n",
    "for key, value in all_binary_ratings.items():\n",
    "    new_value = []\n",
    "    mean = np.mean(value)\n",
    "    std = np.std(value)\n",
    "    for v in value:\n",
    "        if np.abs((v - mean)/std > 1.95):\n",
    "            continue\n",
    "        new_value.append(v)\n",
    "    assert len(new_value) >= 1\n",
    "    binary_ratings[key] = new_value\n",
    "    binary_ratings_mean[key] = np.mean(new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean and std for binary/range ratings across train/funny, train/not_funny, valid/funny, valid/not_funny, test/funny, test/not_funny\n",
    "\n",
    "bin_ratings = {}\n",
    "ran_ratings = {}\n",
    "for fold in ['Train', 'Valid', 'Test']:\n",
    "    for fold1 in ['Funny', 'Not_Funny']:\n",
    "        for file in os.listdir(os.path.join(dataset_dir, fold, fold1)):\n",
    "            key = f\"{fold}_{fold1}\"\n",
    "            if key not in bin_ratings:\n",
    "                bin_ratings[key] = []\n",
    "            bin_ratings[key].append(binary_ratings_mean[file])\n",
    "            if key not in ran_ratings:\n",
    "                ran_ratings[key] = []\n",
    "            ran_ratings[key].append(range_ratings_mean[file])\n",
    "\n",
    "for key, value in bin_ratings.items():\n",
    "    print(f\"{key} binary mean: {np.mean(value)} std: {np.std(value)}\")\n",
    "\n",
    "for key, value in ran_ratings.items():\n",
    "    print(f\"{key} range mean: {np.mean(value)} std: {np.std(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'O767.jpg'\n",
    "binary_ratings_mean[file], range_ratings_mean[file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ratings[file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "for file in train_funny+valid_funny+test_funny:\n",
    "    assert file not in train_not_funny+valid_not_funny+test_not_funny\n",
    "    assert binary_ratings_mean[file] >= 0.5\n",
    "\n",
    "for file in train_not_funny+valid_not_funny+test_not_funny:\n",
    "    assert file not in train_funny+valid_funny+test_funny\n",
    "    assert binary_ratings_mean[file] < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('HumorDB_/range_ratings_mean.txt', 'w') as f:\n",
    "#     for key, value in range_ratings_mean.items():\n",
    "#         f.write(f\"{key},{value}\\n\")\n",
    "\n",
    "# with open('HumorDB_/binary_ratings_mean.txt', 'w') as f:\n",
    "#     for key, value in binary_ratings_mean.items():\n",
    "#         f.write(f\"{key},{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Comparison Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for comp task\n",
    "\n",
    "def extract_ratings_words(filename):\n",
    "    ratings = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[4:]:\n",
    "            line = line.strip().split(',')\n",
    "            ratings.append((line[0], line[1], int(line[2]), line[3]))\n",
    "            # print('Error', filename, key, value)\n",
    "    return ratings\n",
    "\n",
    "compare_representatives = ['M319.jpg',\n",
    " 'M37.jpg',\n",
    " 'O1058.jpg',\n",
    " 'M637.jpg',\n",
    " 'O1709.jpg',\n",
    " 'O1360.jpg',\n",
    " 'O1748.jpg',\n",
    " 'O8.jpg']\n",
    "\n",
    "compare_reps2ids = {comp_rep:i for i, comp_rep in enumerate(compare_representatives)}\n",
    "all_vectors = {}\n",
    "for image in dataset_images:\n",
    "    all_vectors[image] = np.zeros(len(compare_representatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_done = set()\n",
    "comp_dir = 'user_comp'\n",
    "image_words = {}\n",
    "for file in os.listdir(comp_dir):\n",
    "    all_ratings = extract_ratings_words(os.path.join(comp_dir, file))\n",
    "    user_ratings = {}\n",
    "    count_diff = 0\n",
    "    num_repeat = 0\n",
    "    for rating in all_ratings:\n",
    "        img1, img2, comp_res, word = rating\n",
    "        if comp_res == 1:\n",
    "            if img1 not in image_words:\n",
    "                image_words[img1] = []\n",
    "            image_words[img1].append(word)\n",
    "        else:\n",
    "            if img2 not in image_words:\n",
    "                image_words[img2] = []\n",
    "            image_words[img2].append(word)\n",
    "        imgs_key = tuple(sorted([img1, img2]))\n",
    "        if imgs_key[0] != img1:\n",
    "            comp_res = 2 if comp_res == 1 else 1\n",
    "        if imgs_key in user_ratings:\n",
    "            if user_ratings[imgs_key][0] != comp_res:\n",
    "                count_diff += 1\n",
    "            num_repeat += 1\n",
    "        else:\n",
    "            user_ratings[imgs_key] = (comp_res, word)\n",
    "    \n",
    "    if num_repeat <= 0:\n",
    "        print(f\"Number of repeats for {file} is {num_repeat}\")\n",
    "    if count_diff > 3:\n",
    "        print(file, count_diff)\n",
    "    \n",
    "    for key, value in user_ratings.items():\n",
    "        img1, img2 = key\n",
    "        comp_res, word = value\n",
    "        if img1 in compare_representatives and img2 in compare_representatives:\n",
    "            if comp_res == 1:\n",
    "                all_vectors[img1][compare_reps2ids[img2]] += 1\n",
    "                all_vectors[img2][compare_reps2ids[img1]] -= 1\n",
    "            else:\n",
    "                all_vectors[img1][compare_reps2ids[img2]] -= 1\n",
    "                all_vectors[img2][compare_reps2ids[img1]] += 1\n",
    "            all_done.add((img1, img2))\n",
    "            all_done.add((img2, img1))\n",
    "        elif img1 in compare_representatives:\n",
    "            rep_image = img1\n",
    "            other_image = img2\n",
    "            if comp_res == 1:\n",
    "                all_vectors[other_image][compare_reps2ids[rep_image]] -= 1\n",
    "            else:\n",
    "                all_vectors[other_image][compare_reps2ids[rep_image]] += 1\n",
    "            all_done.add((other_image, rep_image))\n",
    "        elif img2 in compare_representatives:\n",
    "            rep_image = img2\n",
    "            other_image = img1\n",
    "            if comp_res == 1:\n",
    "                all_vectors[other_image][compare_reps2ids[rep_image]] += 1\n",
    "            else:\n",
    "                all_vectors[other_image][compare_reps2ids[rep_image]] -= 1\n",
    "            all_done.add((other_image, rep_image))\n",
    "        else:\n",
    "            #dummy data ignore\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "for image in dataset_images:\n",
    "    for rep_image in compare_representatives:\n",
    "        if (image, rep_image) not in all_done and (rep_image, image) not in all_done:\n",
    "            print(f\"Image {image} and {rep_image} not compared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Words Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract common words\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from itertools import tee, zip_longest\n",
    "\n",
    "def common_words(strings, percentage):\n",
    "    # Tokenize and tag parts of speech for each string\n",
    "    tagged_words = []\n",
    "    for string in strings:\n",
    "        words = word_tokenize(string)\n",
    "        tagged_words.extend(pos_tag(words))\n",
    "\n",
    "    # Filter out common nouns and verbs\n",
    "    common_nouns_verbs = [word for word, pos in tagged_words if pos.startswith('NN') or pos.startswith('VB')]\n",
    "    word_counts = nltk.FreqDist(common_nouns_verbs)\n",
    "    threshold = len(strings) * percentage / 100\n",
    "    common = [word for word, count in word_counts.items() if count >= threshold]\n",
    "\n",
    "    return common\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def ngrams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def common_phrases(strings, percentage, n=2):\n",
    "    all_tokens = ' '.join(strings).split()\n",
    "\n",
    "    # Generate n-grams\n",
    "    all_ngrams = ngrams(all_tokens, n)\n",
    "\n",
    "    # Count the occurrences of each n-gram\n",
    "    ngram_counts = Counter(all_ngrams)\n",
    "    threshold = len(strings) * percentage / 100\n",
    "\n",
    "\n",
    "    common_phrases = [phrase for phrase, count in ngram_counts.items() if count >= threshold]\n",
    "\n",
    "    return common_phrases\n",
    "\n",
    "#sanity check\n",
    "strings = [\"hello world\", \"hello everyone\", \"hello there\", \"world is hello\"]\n",
    "percentage = 30  \n",
    "print(common_phrases(strings, percentage))\n",
    "print(common_words(strings, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_common_words = {}\n",
    "count = []\n",
    "non_count = []\n",
    "total = []\n",
    "commoned = 0\n",
    "for image, words in image_words.items():\n",
    "    percentage = 30\n",
    "    common = common_words(words, percentage)\n",
    "    if len(common) > 0:\n",
    "        image_common_words[image] = common\n",
    "        count.append(image)\n",
    "        commoned += len(words)\n",
    "    else:\n",
    "        non_count.append(image)\n",
    "    total.append(image)\n",
    "print(len(image_common_words), len(count), len(total), commoned, len(non_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('image_common_words.txt', 'w') as f:\n",
    "#     for key, value in image_common_words.items():\n",
    "#         f.write(f\"{key}, {' '.join(value)}\\n\")\n",
    "len(image_words['M37.jpg']), len(image_common_words['M37.jpg']), image_common_words['M37.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good to check\n",
    "funny = 0\n",
    "not_funny = 0\n",
    "for image in dataset_images:\n",
    "    if np.mean(binary_ratings[image]) >= 0.5:\n",
    "        if image in image_common_words:\n",
    "            funny += 1\n",
    "    else:\n",
    "        if image in image_common_words:\n",
    "            not_funny += 1\n",
    "funny, not_funny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for comp task map to either 1, -1, 0\n",
    "for image, vector in all_vectors.items():\n",
    "    for i in range(len(vector)):\n",
    "        if vector[i] > 0:\n",
    "            vector[i] = 1\n",
    "        elif vector[i] < 0:\n",
    "            vector[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {}\n",
    "for image, vector in all_vectors.items():\n",
    "    for idx, v in enumerate(vector):\n",
    "        rep_image = compare_representatives[idx]\n",
    "        labels_dict[f\"{image}_{rep_image}\"] = v\n",
    "json.dump(labels_dict, open('labels_dict.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster using Comparison Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gph.python import ripser_parallel\n",
    "\n",
    "# import utils\n",
    "import numpy as np\n",
    "from gtda.homology._utils import _postprocess_diagrams\n",
    "\n",
    "# to generate dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from gtda.plotting import plot_diagram, plot_point_cloud\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make numpy array\n",
    "#for mapping back ids\n",
    "all_vectors_np_ids = {}\n",
    "num_iter = 0\n",
    "for image, vector in all_vectors.items():\n",
    "    all_vectors_np_ids[num_iter] = image\n",
    "    num_iter += 1\n",
    "all_vectors_np = [[] for i in range(len(all_vectors_np_ids))]\n",
    "for i, image in all_vectors_np_ids.items():\n",
    "    all_vectors_np[i] = all_vectors[image]\n",
    "all_vectors_np = np.array(all_vectors_np)\n",
    "print(all_vectors_np.shape, len(all_vectors_np_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros = 0\n",
    "total_possible_ratings = all_vectors_np.shape[0] * all_vectors_np.shape[1]\n",
    "total_correct= 0\n",
    "conflict_ratings = 0\n",
    "for image, vector in all_vectors.items():\n",
    "    for idx in range(len(vector)):\n",
    "        rep_image = compare_representatives[idx]\n",
    "        other_better = int(binary_ratings_mean[image] > binary_ratings_mean[rep_image])\n",
    "        if vector[idx] == 0:\n",
    "            num_zeros += 1\n",
    "            continue\n",
    "        actual_rating = int(vector[idx] > 0)\n",
    "        if vector[idx] > 0:\n",
    "            if binary_ratings_mean[image] < 0.5 and binary_ratings_mean[rep_image] >= 0.5:\n",
    "                conflict_ratings += 1\n",
    "        elif vector[idx] < 0:\n",
    "            if binary_ratings_mean[image] >= 0.5 and binary_ratings_mean[rep_image] < 0.5:\n",
    "                conflict_ratings += 1\n",
    "        else:\n",
    "            raise ValueError\n",
    "        if actual_rating == other_better:\n",
    "            total_correct += 1\n",
    "num_zeros, total_correct, total_possible_ratings, conflict_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the persistence diagram\n",
    "dgm = ripser_parallel(all_vectors_np, maxdim=2, n_threads=8)\n",
    "\n",
    "print(\"Processed dgm\")\n",
    "# comnvert to gtda format\n",
    "dgm_gtda = _postprocess_diagrams([dgm[\"dgms\"]], \"ripser\", (0, 1, 2), np.infty, True)[0]\n",
    "print(\"dgm data\")\n",
    "# plot\n",
    "plot_diagram(dgm_gtda, homology_dimensions=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "# Example data\n",
    "X = all_vectors_np\n",
    "\n",
    "# Normalize data\n",
    "gamma = 0.1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# similarity_matrix = rbf_kernel(X_scaled, gamma=gamma)\n",
    "\n",
    "# Apply spectral clustering\n",
    "sc = SpectralClustering(n_clusters=3, affinity='rbf', gamma=gamma, assign_labels='cluster_qr')\n",
    "labels = sc.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluate clustering quality\n",
    "silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "davies_bouldin = davies_bouldin_score(X_scaled, labels)\n",
    "calinski_harabasz = calinski_harabasz_score(X_scaled, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.3f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
