{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoFeatureExtractor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunnyNotFunnyDataset(Dataset):\n",
    "    def __init__(self, data=[], root_dir=None, transform=None, image_processor=None):\n",
    "        if root_dir[-1] != '/':\n",
    "            root_dir += '/'\n",
    "        self.root_dir = root_dir\n",
    "        classes = []\n",
    "        for file in os.scandir(root_dir):\n",
    "            if file.is_dir():\n",
    "                classes.append(file.name)\n",
    "        data = []\n",
    "        for i, class_name in enumerate(classes):\n",
    "            for file in os.listdir(root_dir+class_name):\n",
    "                data.append((root_dir + class_name + '/'+ file, 1-i))\n",
    "        self.data = data\n",
    "        self.num_classes = len(classes)\n",
    "        self.transform = transform\n",
    "        self.image_processor = image_processor\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.data[index][0])\n",
    "        if self.image_processor:\n",
    "            image = self.image_processor(images=image, return_tensors='pt')\n",
    "            image['pixel_values'] = image['pixel_values'].squeeze(0)\n",
    "            label = self.data[index][1]\n",
    "            label_tensor = torch.zeros(1)\n",
    "            if label == 1:\n",
    "                label_tensor[0] = 1\n",
    "            image['label'] = label_tensor\n",
    "            image['filename'] = self.data[index][0]\n",
    "            return image\n",
    "        if not self.image_processor:\n",
    "            if self.transform:\n",
    "                try:\n",
    "                    image = self.transform()(image)\n",
    "                except:\n",
    "                    image = self.transform(image)\n",
    "        label = self.data[index][1]\n",
    "        label_tensor = torch.zeros(1)\n",
    "        if label == 1:\n",
    "            label_tensor[0] = 1\n",
    "        return {'image_data':image, 'label':label_tensor, 'filename':self.data[index][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_MAP = {0: \"8GiB\", 1: \"8GiB\", 2: \"8GiB\"}\n",
    "\n",
    "model = transformers.AutoModelForImageClassification.from_pretrained(\"google/vit-huge-patch14-224-in21k\", \n",
    "                                                                     device_map='auto', max_memory=GPU_MAP, num_labels=2, ignore_mismatched_sizes=True)\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-huge-patch14-224-in21k\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('/home/vedaant/send/benchmarks/binary_classification/transformers/vit_huge_Funny/model/0.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_transformers = FunnyNotFunnyDataset(root_dir='/home/vedaant/send/NewDataset2/Test' ,image_processor=processor)\n",
    "test_dataloader_transformers = DataLoader(test_dataset_transformers, batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def TestTransformersWithFileNames():\n",
    "    model.eval()\n",
    "    total_size = 0\n",
    "    total_loss = []\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    file_results = {}\n",
    "    file_detailed_results = {}\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader_transformers:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            inputs = data['pixel_values'].to(device)\n",
    "            targets = data[\"label\"].to(device).flatten()\n",
    "            file_names = data['filename']\n",
    "            inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "            targets = targets.type(torch.cuda.FloatTensor)\n",
    "            #print(ids.shape, \"ids\")\n",
    "            batch_size = inputs.size(0)\n",
    "            #assert batch_size == 1, 'To Test with File Names Batch Size in Test DataLoader should be 1'\n",
    "            output = model(inputs).logits\n",
    "            gc.collect()\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "            output = torch.softmax(output, dim=1)\n",
    "            output_list = output.tolist()\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            if (predictions == targets).float().sum().item() > batch_size:\n",
    "                print('error?')\n",
    "            all_preds += predictions.flatten().cpu().detach().tolist()\n",
    "            all_targets += targets.flatten().cpu().detach().tolist()\n",
    "            correct += (predictions == targets).float().sum().item()\n",
    "            results = (predictions == targets)\n",
    "            for i, file_name in enumerate(file_names):\n",
    "                file_results[file_name] = int(results[i].cpu().detach())\n",
    "                file_detailed_results[file_name] = {}\n",
    "                file_detailed_results[file_name]['pred'] = predictions[i]\n",
    "                file_detailed_results[file_name]['out'] = output_list[i][predictions[i]]\n",
    "                file_detailed_results[file_name]['label'] = targets[i]\n",
    "            gc.collect()\n",
    "            del predictions\n",
    "            del targets\n",
    "            del output\n",
    "            torch.cuda.empty_cache()\n",
    "            total_size += batch_size\n",
    "          #gpu_usage()\n",
    "        accuracy = correct/(total_size)\n",
    "    print(\"Total Test Loss: {:.4f}; Test Accuracy: {:.2f}%\".format(np.sum(total_loss), accuracy*100))\n",
    "    return file_results, file_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_file_results, vit_file_detailed_results = TestTransformersWithFileNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2org = json.load(open('/home/vedaant/send/mod2org.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = '/home/vedaant/send/NewDataset2/Test/Funny/'\n",
    "vit_file_results_key_shorten = {}\n",
    "for key, value in vit_file_results.items():\n",
    "    if '/Funny' in key:\n",
    "        vit_file_results_key_shorten[key[len(temp):]] = value\n",
    "    else:\n",
    "        vit_file_results_key_shorten[key[len(temp)+4:]] = value\n",
    "len(vit_file_results_key_shorten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate_pairs = []\n",
    "inaccurate_pairs = []\n",
    "special = []\n",
    "for key, value in mod2org.items():\n",
    "    if key in vit_file_results_key_shorten and value in vit_file_results_key_shorten:\n",
    "        if vit_file_results_key_shorten[key] == 1 and vit_file_results_key_shorten[value] == 1:\n",
    "            accurate_pairs.append((key, value))\n",
    "            continue\n",
    "        if vit_file_results_key_shorten[key] == 0 and vit_file_results_key_shorten[value] == 0:\n",
    "            special.append((key, value))\n",
    "        inaccurate_pairs.append((key, value))\n",
    "len(accurate_pairs), len(inaccurate_pairs), len(special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for key, value in vit_file_results_key_shorten.items():\n",
    "    if 'M' not in key:\n",
    "        if value == 1:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "correct, total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_map(image_path, model, processor):\n",
    "    image = Image.open(image_path)\n",
    "    data = processor(images=image, return_tensors='pt')\n",
    "    inputs = data.pixel_values.to(device)\n",
    "    inputs.to(device)\n",
    "    output= model(pixel_values = inputs, output_attentions=True)\n",
    "    att_mat = output.attentions\n",
    "    att_mat = torch.stack(att_mat).squeeze(1)\n",
    "    att_mat = att_mat.mean(dim=1)\n",
    "    att_mat = att_mat.cpu().detach()\n",
    "    residual_att = torch.eye(att_mat.size(1))\n",
    "    aug_att_mat = att_mat + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "    v = joint_attentions[-1]\n",
    "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "    result = cv2.resize(mask / mask.max(), image.size)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_map(original_img, att_map):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "    ax1.set_title('Original')\n",
    "    ax2.set_title('Attention Map Last Layer')\n",
    "    _ = ax1.imshow(original_img)\n",
    "    attention_plot = ax2.imshow(att_map)\n",
    "    \n",
    "    # Create a colorbar associated with the attention map\n",
    "    plt.colorbar(attention_plot, ax=ax2, orientation='vertical', fraction=0.036, pad=0.03)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write function to display an image\n",
    "\n",
    "def display_image(image):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "\n",
    "def display_images_in_grid(image_paths, labels, grid_size=None):\n",
    "    \"\"\"\n",
    "    Displays a list of images in a grid with labels under each image.\n",
    "\n",
    "    :param image_paths: List of paths to image files.\n",
    "    :param labels: List of labels for each image.\n",
    "    :param grid_size: Tuple indicating the grid size (rows, columns). If None, a square grid will be used.\n",
    "    \"\"\"\n",
    "    if len(image_paths) != len(labels):\n",
    "        raise ValueError(\"The number of images must match the number of labels\")\n",
    "\n",
    "    if grid_size is None:\n",
    "        n = math.ceil(math.sqrt(len(image_paths)))  # Calculate the size of the square grid\n",
    "        grid_size = (n, n)\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(grid_size[1] * 5, grid_size[0] * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, img_path, label in zip(axes, image_paths, labels):\n",
    "        img = mpimg.imread(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  # Hide the axis\n",
    "        ax.set_title(label, fontsize=12)  # Set the title (label) below the image\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for ax in axes[len(image_paths):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_paths = [os.path.join('/home/vedaant/send/NewDataset2/Test/Funny/', pair[1]) if os.path.exists(os.path.join('/home/vedaant/send/NewDataset2/Test/Funny/', pair[1])) else os.path.join('/home/vedaant/send/NewDataset2/Test/Not_Funny/', pair[1]) for pair in accurate_pairs if os.path.exists(os.path.join('/home/vedaant/send/NewDataset2/Test/Not_Funny/', pair[1])) for pair in accurate_pairs]\n",
    "all_labels = [f\"{i}\" for i in range(len(all_image_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "cutoff = 16\n",
    "image_paths = all_image_paths[start:cutoff]\n",
    "labels = all_labels[start:cutoff]\n",
    "display_images_in_grid(image_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join('/home/vedaant/send/NewDataset2/Test/Not_Funny/', accurate_pairs[9][0])\n",
    "attn_map = get_attention_map(image_path, model, processor)\n",
    "image = Image.open(image_path)\n",
    "attn_map_img = attn_map[..., np.newaxis]\n",
    "attn_map_img = (attn_map_img * image).astype(\"uint8\")\n",
    "plot_attention_map(image, attn_map)\n",
    "# display_image(attn_map_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join('/home/vedaant/send/NewDataset2/Test/Funny/', accurate_pairs[9][1])\n",
    "attn_map = get_attention_map(image_path, model, processor)\n",
    "image = Image.open(image_path)\n",
    "attn_map_img = attn_map[..., np.newaxis]\n",
    "attn_map_img = (attn_map_img * image).astype(\"uint8\")\n",
    "plot_attention_map(image, attn_map)\n",
    "# display_image(attn_map_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_map(image, attn_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funnydataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
